{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee90a23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 things: similarity search w/ embeddings; clustering/topic modelling to understand what kind of documents and use cases our customers use Juro for\n",
    "\n",
    "\n",
    "# 1. prep the data\n",
    "# 2. load the data â€” docs\n",
    "# * selecting a model to do embeddings (BERT, BERTLegal, OpenAI Embedding)\n",
    "# 3. tokenize the docs\n",
    "# 4. use the model to create embeddings out of docs\n",
    "# 5. store the embeddings\n",
    "# * selecting a vector store (pgvector Postgres, MongoDB, ChromaDB, Pinecone)\n",
    "# 6. similarity search, convert incoming doc to embedding and comapring\n",
    "# 7. clustering and topic modelling, figuring out what kind of tasks and docs our users upload and create in Juro\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81125703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "011f2138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET DATA \n",
    "# (this part would normally need quite a lot of attention, with preprocessing, cleaning, trimming, etc)\n",
    "# for simplicity, let's assume that the test dataset is in a csv file, with every row(column string) being a doc to process\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# load the CSV file\n",
    "csv_file_path = 'data/string.csv'  \n",
    "data = pd.read_csv(csv_file_path)\n",
    "\n",
    "# get data from the column that has the doc\n",
    "text_column = 'String'\n",
    "if text_column in data.columns:\n",
    "    document_array = data[text_column].astype(str).tolist()\n",
    "else:\n",
    "    print(f\"Column {text_column} not found in the CSV file.\")\n",
    "    document_array = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28f04a53",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at legal-bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# GENERATE EMBEDDINGS\n",
    "# LegalBERT\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "# path to your custom model directory\n",
    "model_directory = \"legal-bert-base-uncased\"\n",
    "\n",
    "# load the tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(model_directory)\n",
    "model = BertModel.from_pretrained(model_directory)\n",
    "\n",
    "# function to generate embeddings for a list of documents\n",
    "def generate_embeddings(documents):\n",
    "    embeddings = []\n",
    "\n",
    "    for doc in documents:\n",
    "        # tokenize the document\n",
    "        # breaking down big docs into chunks => paragraphs\n",
    "        inputs = tokenizer(doc, return_tensors=\"pt\", truncation=True, max_length=512, padding=\"max_length\")\n",
    "\n",
    "        # get the output from the BERT model\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # extract embeddings\n",
    "        doc_embedding = outputs.last_hidden_state.mean(dim=1)\n",
    "        embeddings.append(doc_embedding)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "document_embeddings = generate_embeddings(document_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b5903af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE EMBEDDINGS\n",
    "#\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "document_embeddings_homo = [embedding.squeeze().tolist() for embedding in document_embeddings]\n",
    "\n",
    "np.save('data/embeddings.npy', document_embeddings_homo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78508276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD EMBEDDINGS\n",
    "#\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "loaded_embeddings = np.load('data/embeddings.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4c73fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at legal-bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document at index 8207:\n",
      "counterparty\n",
      "----------\n",
      "Document at index 33716:\n",
      "counterparty name\n",
      "----------\n",
      "Document at index 13242:\n",
      "Viral signup per counterparty\n",
      "----------\n",
      "Document at index 26528:\n",
      "counterparties\n",
      "----------\n",
      "Document at index 19706:\n",
      "counterparty type \n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# DO SIMILARITY SEARCH\n",
    "#\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# path to your custom model directory\n",
    "model_directory = \"legal-bert-base-uncased\"\n",
    "\n",
    "# load the tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(model_directory)\n",
    "model = BertModel.from_pretrained(model_directory)\n",
    "\n",
    "\n",
    "# generate embedding for input doc\n",
    "def calculate_embedding(doc):\n",
    "    embeddings = []\n",
    "\n",
    "    # Tokenize the document\n",
    "    inputs = tokenizer(doc, return_tensors=\"pt\", truncation=True, max_length=512, padding=\"max_length\")\n",
    "\n",
    "    # Get the output from the BERT model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Extract embeddings\n",
    "    doc_embedding = outputs.last_hidden_state.mean(dim=1)\n",
    "    \n",
    "\n",
    "    return doc_embedding\n",
    "\n",
    "# a naive approach to comparing vectors, this would be performance optimised in vector storage systems as a function\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    # Ensure the vectors are numpy arrays\n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "\n",
    "    # Calculate the dot product of the vectors\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "\n",
    "    # Calculate the magnitude (norm) of each vector\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "\n",
    "    # Compute the cosine similarity\n",
    "    similarity = dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "    return similarity\n",
    "\n",
    "# let's glue it all together, comparing new vector against all stored and selecting top 10 matching\n",
    "def find_most_similar(new_document, stored_embeddings, top_n=5):\n",
    "    # Calculate the embedding for the new document\n",
    "    new_embedding = calculate_embedding(new_document)\n",
    "\n",
    "    # Dictionary to hold document index and its similarity score\n",
    "    similarity_scores = {}\n",
    "\n",
    "    # Compute similarity of the new document against each stored embedding\n",
    "    for index, stored_embedding in enumerate(stored_embeddings):\n",
    "        similarity = cosine_similarity(new_embedding, stored_embedding)\n",
    "        similarity_scores[index] = similarity\n",
    "\n",
    "    # Sort the documents based on similarity scores\n",
    "    sorted_docs = sorted(similarity_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Return the indices of the top N similar documents\n",
    "    return [doc_index for doc_index, _ in sorted_docs[:top_n]]\n",
    "\n",
    "# find similar documents\n",
    "top_similar_indices = find_most_similar(\"counterparty\", loaded_embeddings)\n",
    "\n",
    "for index in top_similar_indices:\n",
    "        if 0 <= index < len(document_array):\n",
    "            print(f\"Document at index {index}:\")\n",
    "            print(document_array[index])\n",
    "            print(\"----------\")\n",
    "        else:\n",
    "            print(f\"Index {index} is out of bounds for the document array.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f217cb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "loaded_embeddings = np.load('data/embeddings.npy')\n",
    "\n",
    "# CLUSTER\n",
    "# dbscan\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute the cosine similarity matrix\n",
    "similarity_matrix = cosine_similarity(loaded_embeddings)\n",
    "\n",
    "# Apply DBSCAN\n",
    "# Note: As DBSCAN works with distances, and cosine similarity is a measure of similarity, \n",
    "# you might need to convert similarities to distances.\n",
    "distance_matrix = 1 - similarity_matrix\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5, metric=\"precomputed\")\n",
    "clusters = dbscan.fit_predict(distance_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2dc7eeac",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clusters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Scatter plot of the reduced data. You can color points by cluster label if available.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# For example, if you have DBSCAN cluster labels, you can use `c=clusters`\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(embeddings_2d[:, \u001b[38;5;241m0\u001b[39m], embeddings_2d[:, \u001b[38;5;241m1\u001b[39m], c\u001b[38;5;241m=\u001b[39mclusters, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrainbow\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m plt\u001b[38;5;241m.\u001b[39mcolorbar()\n\u001b[1;32m     14\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt-SNE feature 1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clusters' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# VISUALIZE\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# reduce dimensionality\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "embeddings_2d = tsne.fit_transform(loaded_embeddings)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "# Scatter plot of the reduced data. You can color points by cluster label if available.\n",
    "# For example, if you have DBSCAN cluster labels, you can use `c=clusters`\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=clusters, cmap='rainbow')\n",
    "plt.colorbar()\n",
    "plt.xlabel('t-SNE feature 1')\n",
    "plt.ylabel('t-SNE feature 2')\n",
    "plt.title('t-SNE visualization of document embeddings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7883cfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID TOPICS BASED ON CLUSTERS\n",
    "import numpy as np\n",
    "\n",
    "# compute centroids\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# to analyze representative terms, we need to find the closest words to each centroid\n",
    "# this is a bit more complex, it requires mapping back from the embedding space to the word space\n",
    "# a simplistic approach is to find the documents closest to each centroid\n",
    "for i, centroid in enumerate(centroids):\n",
    "    distances = np.linalg.norm(reshaped_embeddings - centroid, axis=1)\n",
    "    closest_doc_index = np.argmin(distances)\n",
    "    print(f\"Cluster {i} representative document: {document_array[closest_doc_index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbdae77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOPIC REVIEW AND FEATURE EXTRACTION FROM DOCUMENTS\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# create TF-IDF representation of the documents\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, min_df=2, stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(document_array)\n",
    "\n",
    "# function to extract top N keywords from each cluster\n",
    "def extract_top_keywords_per_cluster(tfidf_matrix, cluster_assignments, n_top_keywords=20):\n",
    "    keywords_per_cluster = {}\n",
    "    feature_names = np.array(tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "    for cluster_num in range(num_clusters):\n",
    "        # indices of documents in the current cluster\n",
    "        document_indices = np.where(cluster_assignments == cluster_num)[0]\n",
    "\n",
    "        # aggregate TF-IDF scores of these documents\n",
    "        if isinstance(tfidf_matrix, np.matrix):\n",
    "            aggregated_tfidf = np.mean(tfidf_matrix[document_indices], axis=0).A1  # Use .A1 for numpy matrix\n",
    "        else:\n",
    "            aggregated_tfidf = np.mean(tfidf_matrix[document_indices].toarray(), axis=0)  # Use .toarray() for sparse matrix\n",
    "\n",
    "        # get top N keywords\n",
    "        top_keyword_indices = aggregated_tfidf.argsort()[-n_top_keywords:][::-1]\n",
    "        top_keywords = feature_names[top_keyword_indices]\n",
    "        keywords_per_cluster[cluster_num] = top_keywords\n",
    "\n",
    "    return keywords_per_cluster\n",
    "\n",
    "# extract and print top keywords for each cluster\n",
    "top_keywords_per_cluster = extract_top_keywords_per_cluster(tfidf_matrix, cluster_assignments)\n",
    "for cluster, keywords in top_keywords_per_cluster.items():\n",
    "    print(f\"Cluster {cluster}: {', '.join(keywords)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39af0fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZE CLUSTERS w TOPIC NAMES\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# prepare hover text (first 100 characters of each document)\n",
    "hover_texts = [\"Doc \" + str(i) + \": \" + doc[:100] + \"...\" for i, doc in enumerate(document_array)]\n",
    "\n",
    "# create a scatter plot\n",
    "fig = go.Figure()\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    # filter points belonging to the current cluster\n",
    "    indices = [j for j, x in enumerate(cluster_assignments) if x == i]\n",
    "    current_cluster_points = reduced_embeddings[indices]\n",
    "    \n",
    "    cluster_hover_texts = [hover_texts[j] for j in indices]\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=current_cluster_points[:, 0], \n",
    "        y=current_cluster_points[:, 1], \n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=5,\n",
    "            line=dict(\n",
    "                width=1,  \n",
    "                color='black' \n",
    "            )\n",
    "        ),\n",
    "        name=f\"Cluster {i}\",\n",
    "        text=cluster_hover_texts,\n",
    "        hoverinfo='text',\n",
    "        hoverlabel=dict(\n",
    "            bgcolor='white', \n",
    "            bordercolor='black', \n",
    "            font_size=12, \n",
    "            font_family='Arial'  \n",
    "        )\n",
    "    ))\n",
    "    \n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Document Embeddings Clustered with t-SNE\",\n",
    "    xaxis_title=\"t-SNE feature 1\",\n",
    "    yaxis_title=\"t-SNE feature 2\",\n",
    "    legend_title=\"Clusters\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbc138f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
